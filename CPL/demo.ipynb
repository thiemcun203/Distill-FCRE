{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniforge3/envs/fcre/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pc/miniforge3/envs/fcre/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 4/4 [00:01<00:00,  2.79it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.30it/s]\n",
      "Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct were not used when initializing LlamaEncoderModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6465, 0.1617],\n",
      "        [0.0782, 0.5836]])\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.6470, 0.1619],\\n        [0.0786, 0.5844]])\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm2vec import LLM2Vec\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Loading base Mistral model, along with custom code that enables bidirectional connections in decoder-only LLMs. MNTP LoRA weights are merged into the base model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    ")\n",
    "model = model.merge_and_unload()  # This can take several minutes on cpu\n",
    "\n",
    "# Loading supervised model. This loads the trained LoRA weights on top of MNTP model. Hence the final weights are -- Base model + MNTP (LoRA) + supervised (LoRA).\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\"\n",
    ")\n",
    "\n",
    "# Wrapper for encoding and pooling operations\n",
    "l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
    "\n",
    "# Encoding queries using instructions\n",
    "instruction = (\n",
    "    \"Given a web search query, retrieve relevant passages that answer the query:\"\n",
    ")\n",
    "queries = [\n",
    "    [instruction, \"how much protein should a female eat\"],\n",
    "    [instruction, \"summit define\"],\n",
    "]\n",
    "q_reps = l2v.encode(queries)\n",
    "\n",
    "# Encoding documents. Instruction are not required for documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "d_reps = l2v.encode(documents)\n",
    "\n",
    "# Compute cosine similarity\n",
    "q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1)\n",
    "d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1)\n",
    "\n",
    "cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1))\n",
    "\n",
    "print(cos_sim)\n",
    "print(len(d_reps_norm))\n",
    "\"\"\"\n",
    "tensor([[0.6470, 0.1619],\n",
    "        [0.0786, 0.5844]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6465, 0.1617],\n",
      "        [0.0782, 0.5836]])\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim)\n",
    "print(len(d_reps_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniforge3/envs/fcre/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from llm2vec import LLM2Vec\n",
    "from torch import Tensor, device\n",
    "from llm2vec import LLM2Vec\n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from typing import List, Optional\n",
    "\n",
    "def batch_to_device(batch, target_device: device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "class EncodingModel_LLM2Vec(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "            )\n",
    "\n",
    "        self.encoder = LLM2Vec.from_pretrained(\n",
    "            \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "            peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\",\n",
    "            device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            merge_peft=True,\n",
    "            pooling_mode=\"mean\",\n",
    "            max_length=256,\n",
    "            token = \"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\",\n",
    "        )\n",
    "        # print(123)\n",
    "        if config.train_llm2vec == True:\n",
    "            print(type(config.train_llm2vec))\n",
    "            self.encoder.model = self.initialize_peft(\n",
    "                self.encoder.model,\n",
    "            )\n",
    "\n",
    "            \n",
    "    def initialize_peft(\n",
    "        self,\n",
    "        model,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.05,\n",
    "        lora_modules: Optional[List[str]] = None,\n",
    "    ):\n",
    "        if lora_modules is None and model.config.__class__.__name__ in [\n",
    "            \"LlamaConfig\",\n",
    "            \"MistralConfig\",\n",
    "            \"GemmaConfig\",\n",
    "            \"Qwen2Config\",\n",
    "        ]:\n",
    "            lora_modules = [\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "                \"k_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ]\n",
    "        elif lora_modules is None:\n",
    "            raise ValueError(\"lora_modules must be specified for this model.\")\n",
    "\n",
    "        config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=lora_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=None,\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, config)\n",
    "        print(f\"Model's Lora trainable parameters:\")\n",
    "        model.print_trainable_parameters()\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs, is_des = False): # (b, max_length)\n",
    "        # features = self.encoder.tokenize(inputs['input'])\n",
    "        features = self.encoder.tokenize(inputs)\n",
    "        features = batch_to_device(features, self.config.device)\n",
    "        embeddings = self.encoder.forward(features)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/miniforge3/envs/fcre/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00,  4.01it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bool'>\n",
      "Model's Lora trainable parameters:\n",
      "trainable params: 20,971,520 || all params: 7,525,896,192 || trainable%: 0.2787\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "config = Config('config.ini')\n",
    "model = EncodingModel_LLM2Vec(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,  15339,   1917]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0'), 'embed_mask': tensor([[0, 0, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#inference\n",
    "inputs = [\"hello world\"]\n",
    "embeddings = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'input': 'it premiered on cbc in canada on january 7 , 2015 and on bet in the united states on february 16 , 2015 . The relation between \" bet \" and \" united states \" is :', 'relation': 16, 'index': 7978, 'ids': [101, 2009, 5885, 2006, 13581, 1999, 2710, 2006, 2254, 1021, 1010, 2325, 1998, 2006, 6655, 1999, 1996, 2142, 2163, 2006, 2337, 2385, 1010, 2325, 1012, 1, 1, 1, 6655, 1, 1, 1, 103, 1, 1, 1, 2142, 2163, 1, 1, 1, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './result/task_name_num_k-shot.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ave \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(accs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m ave\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./result/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-shot.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhis_acc mean: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39maround(ave, \u001b[38;5;241m4\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/fcre/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './result/task_name_num_k-shot.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "acc_list = [[3,4,2],[3,2,4]]\n",
    "accs = np.array(acc_list)\n",
    "ave = np.mean(accs, axis=0)\n",
    "ave\n",
    "with open(f'./result/{\"task_name\"}_{\"num_k\"}-shot.txt', 'w') as f:\n",
    "    f.write('his_acc mean: ' + str(np.around(ave, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
