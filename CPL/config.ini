[GPT]
gpt_temp = 0.0
key = opnenai-key

[task]
seed = 100
device = cuda
task_name = FewRel
;FewRel, Tacred
; relation_name = ./data/CFRLFewRel/relation_name.txt
; training_data = ./data/CFRLFewRel/CFRLdata_10_100_10_5/train_0.txt
; test_data = ./data/CFRLFewRel/CFRLdata_10_100_10_5/test_0.txt
; valid_data = ./data/CFRLFewRel/CFRLdata_10_100_10_5/valid_0.txt
; rel_index = ./data/CFRLFewRel/rel_index.npy
; rel_cluster_label = ./data/CFRLFewRel/CFRLdata_10_100_10_5/rel_cluster_label_0.npy


[continual]
num_k = 5
; num_k = 5-shot, 10-shot
pattern = hybridprompt
; pattern = marker,hardprompt,softprompt,cls,hybridprompt
total_round = 6
task_length = 8
memory_size = 1

[datageneration]
gen = 1
;gen = data generation open or not
num_gen = 2

[training]
batch_size = 4
;16
dtype = bfloat16
epoch = 10
epoch_mem = 10
lr = 0.00001
num_workers = 2

[contrastive]
margin = 0.3
sample_k = 500
contrastive_temp = 0.1

[softprompt]
tune = all
; tune = prompt, all
prompt_init = 0
; prompt_init = 0: random, 
; prompt_init = 1: is, 
; prompt_init = 2: ! @ # [e1] he is as [MASK] * & % [e2] just do it 
prompt_len = 3
prompt_num = 4

[Encoder]
model = bert
; model = reberta, bert 
llm_path = McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp
train_llm2vec = True
bert_path = google-bert/bert-base-uncased
roberta_path = ./roberta-base
max_length = 256
encoder_output_size = 768
